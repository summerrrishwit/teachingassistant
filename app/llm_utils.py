
import os
from app.config import FRAME_DIR, MODEL  # å¯ç”¨ vLLM æ—¶è¯·åŒæ—¶å¯¼å…¥ VLLM_MODEL_PATH, VLLM_CONFIG
from app.prompts import prompt_dict
import base64

# ============================================================================
# Ollama å¯¼å…¥å’Œåˆå§‹åŒ–
# ============================================================================
ollama = None
try:
    import ollama
    OLLAMA_AVAILABLE = True
except Exception as ollama_import_error:
    OLLAMA_AVAILABLE = False
    ollama_error = str(ollama_import_error)
    ollama = None  # ç¡®ä¿ ollama å˜é‡å­˜åœ¨ï¼Œå³ä½¿å¯¼å…¥å¤±è´¥

# ============================================================================
# vLLM ç›¸å…³ä»£ç å·²æ³¨é‡Šï¼Œæ”¹ç”¨ Ollama
# ============================================================================
# try:
#     from vllm import LLM, SamplingParams
#     VLLM_AVAILABLE = True
# except Exception as vllm_import_error:
#     VLLM_AVAILABLE = False
#     vllm_error = str(vllm_import_error)

import streamlit as st
from typing import List, Dict
from app.rag_utils import VideoRAGSystem


def image_to_base64(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode("utf-8")


def filter_promotional_content(text: str) -> str:
    """
    è¿‡æ»¤æ‰æ¨å¹¿å†…å®¹ã€è®¢é˜…é“¾æ¥ã€newsletterç­‰ä¿¡æ¯
    ç²¾ç¡®åŒ¹é…ç‰¹å®šçš„æ¨å¹¿å†…å®¹ï¼Œé¿å…è¯¯åˆ æœ‰ç”¨ä¿¡æ¯
    """
    import re
    
    if not text:
        return text
    
    # å®šä¹‰éœ€è¦è¿‡æ»¤çš„ç²¾ç¡®æ¨¡å¼ï¼ˆæ›´å…·ä½“çš„æ¨å¹¿å†…å®¹ï¼‰
    promotional_patterns = [
        # ç‰¹å®šçš„ç½‘ç«™å’ŒåŸŸå
        r'blog\.bybigo\.com',
        r'bybigo\.com',
        # newsletterç›¸å…³
        r'subscribe.*newsletter',
        r'newsletter.*subscribe',
        r'system design newsletter',
        # ç‰¹å®šçš„æ¨å¹¿æ–‡æœ¬
        r'If you like our videos.*we might like.*newsletter',
        r'trusted by.*\d+.*readers',
        r'subscribe to blog\.',
        r'subscribe to.*blog',
        # URLæ¨¡å¼ï¼ˆä½†åªè¿‡æ»¤æ˜æ˜¾çš„æ¨å¹¿é“¾æ¥ï¼‰
        r'http[s]?://[^\s]*blog[^\s]*',
        r'http[s]?://[^\s]*newsletter[^\s]*',
        r'http[s]?://[^\s]*subscribe[^\s]*',
    ]
    
    # æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            filtered_lines.append(line)  # ä¿ç•™ç©ºè¡Œä»¥ç»´æŒæ ¼å¼
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨å¹¿å†…å®¹
        is_promotional = False
        matched_pattern = None
        
        for pattern in promotional_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                is_promotional = True
                matched_pattern = pattern
                break
        
        if not is_promotional:
            # ä¸åŒ…å«æ¨å¹¿å†…å®¹ï¼Œä¿ç•™è¯¥è¡Œ
            filtered_lines.append(line)
        else:
            # åŒ…å«æ¨å¹¿å†…å®¹ï¼Œå°è¯•æå–æœ‰ç”¨éƒ¨åˆ†
            if matched_pattern:
                match = re.search(matched_pattern, line, re.IGNORECASE)
                if match:
                    # ä¿ç•™æ¨å¹¿å†…å®¹ä¹‹å‰çš„éƒ¨åˆ†
                    useful_part = line[:match.start()].strip()
                    # å¦‚æœå‰é¢æœ‰æœ‰ç”¨å†…å®¹ï¼ˆé•¿åº¦åˆç†ï¼‰ï¼Œä¿ç•™å®ƒ
                    if useful_part and len(useful_part) > 10:
                        filtered_lines.append(useful_part)
                    # å¦åˆ™å®Œå…¨è·³è¿‡è¿™ä¸€è¡Œ
    
    # é‡æ–°ç»„åˆæ–‡æœ¬
    filtered_text = '\n'.join(filtered_lines)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œï¼ˆä¿ç•™å•ä¸ªç©ºè¡Œï¼Œåˆ é™¤è¿ç»­å¤šä¸ªç©ºè¡Œï¼‰
    filtered_text = re.sub(r'\n{3,}', '\n\n', filtered_text)
    
    # ç§»é™¤æ–‡æœ¬æœ«å°¾çš„æ¨å¹¿å†…å®¹
    # æ£€æŸ¥æœ€åå‡ è¡Œæ˜¯å¦åŒ…å«æ¨å¹¿å†…å®¹
    final_lines = filtered_text.split('\n')
    while final_lines:
        last_line = final_lines[-1].strip().lower()
        # å¦‚æœæœ€åä¸€è¡ŒåŒ…å«æ¨å¹¿å…³é”®è¯ï¼Œç§»é™¤å®ƒ
        if any(re.search(pattern, last_line, re.IGNORECASE) for pattern in promotional_patterns):
            final_lines.pop()
        else:
            break
    
    filtered_text = '\n'.join(final_lines)
    
    return filtered_text.strip()


from langchain_huggingface import HuggingFaceEmbeddings

# ============================================================================
# Ollama æ¨¡å‹åˆå§‹åŒ–
# ============================================================================

def check_ollama_connection():
    """æ£€æŸ¥Ollamaè¿æ¥"""
    if not OLLAMA_AVAILABLE or ollama is None:
        error_detail = ollama_error if 'ollama_error' in locals() or 'ollama_error' in globals() else 'æœªçŸ¥é”™è¯¯'
        raise RuntimeError(
            f"âŒ Ollama æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {error_detail}\n\n"
            "è¯·å®‰è£… Ollama: pip install ollama\n"
            "å¹¶ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve"
        )
    
    try:
        # æµ‹è¯•è¿æ¥
        ollama.list()
        return True
    except Exception as e:
        raise RuntimeError(
            f"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡: {e}\n\n"
            "è¯·ç¡®ä¿ï¼š\n"
            f"1. Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve\n"
            f"2. æ¨¡å‹å·²ä¸‹è½½: ollama pull {MODEL}\n"
            "3. æ£€æŸ¥ç½‘ç»œè¿æ¥"
        )

# ============================================================================
# vLLM æ¨¡å‹åˆå§‹åŒ–ï¼ˆå·²æ³¨é‡Šï¼Œæ”¹ç”¨ Ollamaï¼‰
# ============================================================================
# @st.cache_resource
# def get_vllm_model():
#     """
#     è·å–å¹¶ç¼“å­˜ vLLM æ¨¡å‹å®ä¾‹
#     æ”¯æŒ Flash Attention åŠ é€Ÿ
#
#     å¤šå¡åœºæ™¯æç¤ºï¼š
#     - tensor_parallel_size å»ºè®®è®¾ä¸º GPU æ•°é‡ï¼ˆä¾‹å¦‚ torch.cuda.device_count()ï¼‰
#     - å•å¡æ—¶ä¿æŒ 1ï¼Œå¤šå¡æ—¶æŒ‰éœ€è°ƒæ•´ max_model_len / max_num_seqs
#     - å¤šå¡ä¸‹æ¯å¡çš„ gpu_memory_utilization éœ€æ›´ä¿å®ˆï¼Œé¿å… profile_run OOM
#     - å¤šæ¨¡æ€æ¨¡å‹éœ€è¦æ›´å¤šæ˜¾å­˜å¤„ç†å›¾åƒè¾“å…¥ï¼Œå»ºè®®å…ˆç”¨å°åˆ†è¾¨ç‡å›¾åƒéªŒè¯
#     """
#     if not VLLM_AVAILABLE:
#         raise RuntimeError(
#             f"âŒ vLLM æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {vllm_error}\n\n"
#             "è¯·å®‰è£… vLLM: pip install vllm\n"
#             "æ³¨æ„ï¼švLLM éœ€è¦ CUDA å’Œ GPU æ”¯æŒ"
#         )
#     
#     try:
#         # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
#         import torch
#         if not torch.cuda.is_available():
#             raise RuntimeError(
#                 "âŒ CUDA ä¸å¯ç”¨ã€‚vLLM éœ€è¦ NVIDIA GPU å’Œ CUDA æ”¯æŒã€‚\n"
#                 "è¯·ç¡®ä¿ï¼š\n"
#                 "1. å·²å®‰è£… NVIDIA GPU é©±åŠ¨\n"
#                 "2. å·²å®‰è£… CUDA toolkit\n"
#                 "3. PyTorch æ”¯æŒ CUDA"
#             )
#         
#         # è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…å™¨é…ç½®ï¼ˆé¿å…å†…å­˜ç¢ç‰‡ï¼‰
#         # è¿™æœ‰åŠ©äºå‡å°‘ OOM é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶
#         if not os.getenv("PYTORCH_CUDA_ALLOC_CONF"):
#             os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
#         
#         # è®¾ç½® HuggingFace é•œåƒæºï¼ˆè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜ï¼‰
#         # å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨å›½å†…é•œåƒæº
#         if not os.getenv("HF_ENDPOINT"):
#             os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
#         
#         # å¦‚æœè®¾ç½®äº† HF_TOKENï¼Œä½¿ç”¨å®ƒ
#         hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACEHUB_API_TOKEN")
#         if hf_token:
#             os.environ["HF_TOKEN"] = hf_token
#             os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token
#         
#         # ä»é…ç½®è·å–æ¨¡å‹è·¯å¾„å’Œå‚æ•°
#         model_path = VLLM_MODEL_PATH
#         if not model_path:
#             raise RuntimeError("æœªè®¾ç½® VLLM_MODEL_PATHï¼Œè¯·åœ¨ app/config.py ä¸­é…ç½®")
#         if os.path.sep in model_path and not os.path.exists(model_path):
#             raise RuntimeError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
#
#         config = VLLM_CONFIG.copy()
#         
#         # å¤šå¡åœºæ™¯å»ºè®®ï¼ˆå¯é€‰ï¼‰
#         # å¦‚æœæœªåœ¨ VLLM_CONFIG ä¸­æ˜¾å¼è®¾ç½®ï¼Œå¯æŒ‰ GPU æ•°é‡è¦†ç›–
#         # gpu_count = torch.cuda.device_count()
#         # if gpu_count > 1 and not config.get("tensor_parallel_size"):
#         #     config["tensor_parallel_size"] = gpu_count
#         
#         st.info(f"ğŸš€ æ­£åœ¨åŠ è½½ vLLM æ¨¡å‹: {model_path}\n"
#                 f"é…ç½®: GPUåˆ©ç”¨ç‡={config.get('gpu_memory_utilization', 0.85)}, "
#                 f"æœ€å¤§é•¿åº¦={config.get('max_model_len', 8192)}\n"
#                 f"HuggingFace é•œåƒ: {os.getenv('HF_ENDPOINT', 'é»˜è®¤')}")
#         
#         # åˆ›å»º vLLM å®ä¾‹
#         llm = LLM(
#             model=model_path,
#             trust_remote_code=True,
#             **config
#         )
#         
#         st.success("âœ… vLLM æ¨¡å‹åŠ è½½æˆåŠŸï¼Flash Attention å·²å¯ç”¨")
#         return llm
#         
#     except Exception as e:
#         error_msg = str(e)
#         
#         # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆ
#         error_lower = error_msg.lower()
#         solutions = []
#         
#         if "not a local folder" in error_lower or "valid model identifier" in error_lower:
#             solutions.append("1. **æ¨¡å‹è·¯å¾„é—®é¢˜**ï¼š")
#             solutions.append("   - å¦‚æœä½¿ç”¨ HuggingFace æ¨¡å‹ IDï¼Œç¡®ä¿ç½‘ç»œå¯ä»¥è®¿é—® huggingface.co")
#             solutions.append("   - æˆ–è€…å…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶åä½¿ç”¨æœ¬åœ°è·¯å¾„")
#         
#         if "cuda" in error_lower or "gpu" in error_lower:
#             solutions.append("2. **GPU/CUDA é—®é¢˜**ï¼š")
#             solutions.append("   - è¿è¡Œ `nvidia-smi` æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨")
#             solutions.append("   - æ£€æŸ¥ CUDA ç‰ˆæœ¬ï¼š`nvcc --version`")
#         
#         if "memory" in error_lower or "out of memory" in error_lower:
#             solutions.append("3. **æ˜¾å­˜ä¸è¶³**ï¼š")
#             solutions.append("   - é™ä½ max_model_lenï¼ˆå¦‚ 2048 æˆ– 4096ï¼‰")
#             solutions.append("   - é™ä½ gpu_memory_utilizationï¼ˆå¦‚ 0.40 æˆ– 0.50ï¼‰")
#             solutions.append("   - æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨ GPU æ˜¾å­˜ï¼š`nvidia-smi`")
#             solutions.append("   - æ¸…ç† GPU æ˜¾å­˜ï¼š`kill -9 <å ç”¨æ˜¾å­˜çš„è¿›ç¨‹PID>`")
#             solutions.append("   - å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True æ¥å‡å°‘å†…å­˜ç¢ç‰‡")
#         
#         if not solutions:
#             solutions.append("1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
#             solutions.append("2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆå¦‚æœä½¿ç”¨ HuggingFace æ¨¡å‹ï¼‰")
#         
#         st.error(
#             f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}\n\n"
#             "**è§£å†³æ–¹æ¡ˆï¼š**\n" + "\n".join(solutions)
#         )
#         raise


@st.cache_resource
def get_embedding_model():
    """è·å–å¹¶ç¼“å­˜Embeddingæ¨¡å‹"""
    import os
    
    # è®¾ç½® HuggingFace é•œåƒæºï¼ˆè§£å†³401é”™è¯¯ï¼‰
    if not os.getenv("HF_ENDPOINT"):
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    
    hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACEHUB_API_TOKEN")
    
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        return embedding_model
    except Exception as e:
        error_str = str(e)
        if "401" in error_str or "Unauthorized" in error_str:
            st.warning("âš ï¸ æ¨¡å‹ä¸‹è½½é‡åˆ°è®¤è¯é—®é¢˜ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
            try:
                os.environ["HF_ENDPOINT"] = "https://huggingface.co"
                embedding_model = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                return embedding_model
            except Exception as e2:
                st.error(
                    f"âŒ æ— æ³•ä¸‹è½½æ¨¡å‹: {e2}\n\n"
                    "è§£å†³æ–¹æ¡ˆï¼š\n"
                    "1. è®¾ç½® HuggingFace token: export HF_TOKEN=your_token\n"
                    "2. æˆ–è€…ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹\n"
                )
                raise
        else:
            raise


def get_rag_system():
    """è·å–RAGç³»ç»Ÿå®ä¾‹"""
    embedding_model = get_embedding_model()
    return VideoRAGSystem(embedding_model)


def contextualize_query(query: str, history: List[Dict]) -> str:
    """
    æ ¹æ®å¯¹è¯å†å²é‡å†™æŸ¥è¯¢ï¼Œä½¿å…¶ç‹¬ç«‹åŒ–
    ä½¿ç”¨ Ollama è¿›è¡ŒæŸ¥è¯¢é‡å†™
    """
    if not history:
        return query
        
    if not OLLAMA_AVAILABLE:
        return query

    # æ„å»ºé‡å†™æç¤ºè¯
    conversation_str = ""
    for msg in history[-4:]:  # åªçœ‹æœ€è¿‘å‡ è½®
        role = msg.get("role")
        content = msg.get("content")
        conversation_str += f"{role}: {content}\n"
    
    prompt = f"""Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.

Chat History:
{conversation_str}

User Question: {query}

Standalone Question:"""

    try:
        # ä½¿ç”¨ Ollama è¿›è¡ŒæŸ¥è¯¢é‡å†™
        if ollama is None:
            return query
        response = ollama.chat(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        rewritten_query = response["message"]["content"].strip()
        
        # ç®€å•çš„éªŒè¯ï¼šå¦‚æœè¿”å›å¤ªé•¿æˆ–è€…æ˜¯åºŸè¯ï¼Œå›é€€åˆ°åŸé—®é¢˜
        if len(rewritten_query) > len(query) * 2 and len(query) > 10:
             return query
        return rewritten_query
    except Exception as e:
        print(f"Query rewriting failed: {e}")
        return query


def get_response(text, question, full_transcript, prompt_key, summarized_transcript, 
                 segments=None, use_rag=True, frame_paths=None):
    """
    è·å–LLMå“åº”ï¼Œæ”¯æŒRAGå¢å¼º
    ä½¿ç”¨ Ollama è¿›è¡Œå›ç­”
    """
    # æ£€æŸ¥ Ollama æ˜¯å¦å¯ç”¨
    if not OLLAMA_AVAILABLE or ollama is None:
        error_detail = ollama_error if 'ollama_error' in locals() or 'ollama_error' in globals() else 'æœªçŸ¥é”™è¯¯'
        raise RuntimeError(
            f"âŒ Ollama æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {error_detail}\n\n"
            "è¯·å®‰è£… Ollama: pip install ollama\n"
            "å¹¶ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve"
        )
    
    # æ£€æŸ¥ Ollama è¿æ¥
    try:
        check_ollama_connection()
    except Exception as e:
        raise RuntimeError(str(e))

    prompt = prompt_dict[prompt_key]

    # å‡†å¤‡å¯¹è¯å†å²ç”¨äºæŸ¥è¯¢é‡å†™
    history_messages = []
    if "qa_conversation_history" in st.session_state and st.session_state.qa_conversation_history:
        for item in st.session_state.qa_conversation_history[-6:]:
            if item.get("question"):
                history_messages.append({"role": "user", "content": item.get("question")})
            if item.get("answer"):
                history_messages.append({"role": "assistant", "content": item.get("answer")})

    # RAGæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    retrieved_contexts = []
    if use_rag and question and segments:
        try:
            if "rag_system" not in st.session_state:
                st.session_state.rag_system = get_rag_system()
            rag_system = st.session_state.rag_system
            
            if rag_system.vectorstore is None:
                with st.spinner("ğŸ” æ„å»ºå‘é‡ç´¢å¼•..."):
                    rag_system.build_vector_store(segments)
            
            search_query = question
            if history_messages:
                search_query = contextualize_query(question, history_messages)
            
            retrieved_contexts = rag_system.retrieve_relevant_context(
                search_query, 
                top_k=3
            )
        except Exception as e:
            st.warning(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡: {e}")
            retrieved_contexts = []

    prompt_inputs = []
    image_sources = frame_paths if frame_paths is not None else [
        os.path.join(FRAME_DIR, path)
        for path in os.listdir(FRAME_DIR)
        if path.endswith(".jpg")
    ]
    for image_path in image_sources:
        if os.path.exists(image_path):
            prompt_inputs.append(image_to_base64(image_path))

    # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœä½¿ç”¨RAGï¼‰
    if prompt_key == "video_qa":
        if retrieved_contexts and use_rag:
            retrieved_info = "\n\n".join([
                f"[æ—¶é—´æˆ³: {ctx['timestamp']}] {ctx['text']}"
                for ctx in retrieved_contexts
            ])
            enhanced_context = f"{summarized_transcript}\n\n=== ç›¸å…³ç‰‡æ®µï¼ˆåŸºäºè¯­ä¹‰æ£€ç´¢ï¼‰ ===\n{retrieved_info}"
            
            if "video_qa_rag" in prompt_dict:
                prompt = prompt_dict["video_qa_rag"].format(
                    text=text, 
                    question=question, 
                    global_context=summarized_transcript,
                    retrieved_contexts=retrieved_info
                )
            else:
                prompt = prompt.format(
                    text=text, 
                    question=question, 
                    global_context=enhanced_context
                )
        else:
            prompt = prompt.format(text=text, question=question, global_context=summarized_transcript)
    elif prompt_key == "bullet_points":
        prompt = prompt.format(text=full_transcript)
    elif prompt_key == "qa_style":
        prompt = prompt.format(text=full_transcript)
    elif prompt_key == "video_summary":
        prompt = prompt.format(text=full_transcript)

    # æ„å»ºå¤šè½®å¯¹è¯æ¶ˆæ¯
    messages = []
    
    conversation_pairs = []
    try:
        if "qa_conversation_history" in st.session_state and st.session_state.qa_conversation_history:
            for item in st.session_state.qa_conversation_history[-6:]:
                conversation_pairs.append((item.get("question", ""), item.get("answer", "")))
    except Exception:
        conversation_pairs = []

    # æ·»åŠ å¯¹è¯å†å²
    if conversation_pairs:
        for (prev_q, prev_a) in conversation_pairs:
            if prev_q:
                messages.append({"role": "user", "content": prev_q})
            if prev_a:
                messages.append({"role": "assistant", "content": prev_a})
    
    # æ„å»ºå½“å‰æ¶ˆæ¯
    # æ³¨æ„ï¼šOllama æ”¯æŒå›¾åƒè¾“å…¥ï¼Œä½†éœ€è¦æ¨¡å‹æ”¯æŒå¤šæ¨¡æ€
    # å¦‚æœ prompt_inputs å­˜åœ¨ï¼Œå¯ä»¥æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    current_prompt = prompt
    if prompt_inputs:
        # å¦‚æœæœ‰å›¾åƒï¼Œå°è¯•ä½¿ç”¨å¤šæ¨¡æ€ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        # å¦åˆ™åœ¨æç¤ºè¯ä¸­è¯´æ˜æœ‰å›¾åƒä¿¡æ¯
        current_prompt += f"\n\n[æ³¨æ„ï¼šæœ¬æ¬¡æŸ¥è¯¢åŒ…å« {len(prompt_inputs)} ä¸ªè§†é¢‘å¸§å›¾åƒã€‚è¯·åŸºäºè§†é¢‘å¸§çš„æ–‡æœ¬æè¿°å’Œè½¬å½•å†…å®¹è¿›è¡Œç»¼åˆåˆ†æã€‚]"
    
    messages.append({
        "role": "user",
        "content": current_prompt
    })

    full_answer = ""
    placeholder = None
    
    try:
        placeholder = st.empty()
    except Exception:
        pass

    def safe_update_placeholder(content, show_cursor=False):
        """å®‰å…¨åœ°æ›´æ–°å ä½ç¬¦å†…å®¹"""
        if placeholder is not None:
            try:
                if show_cursor:
                    placeholder.markdown(content + "â–Œ")
                else:
                    placeholder.markdown(content)
            except Exception:
                st.markdown(content)

    try:
        # ä½¿ç”¨ Ollama è¿›è¡Œæ¨ç†
        if ollama is None:
            raise RuntimeError("Ollama æ¨¡å—æœªæ­£ç¡®å¯¼å…¥")
        
        # å°è¯•æµå¼è¾“å‡º
        try:
            stream = ollama.chat(
                model=MODEL,
                messages=messages,
                stream=True
            )
            
            for chunk in stream:
                try:
                    if chunk.get("message") and chunk["message"].get("content"):
                        new_text = chunk["message"]["content"]
                        full_answer += new_text
                        safe_update_placeholder(full_answer, show_cursor=True)
                except Exception:
                    continue

            full_answer = filter_promotional_content(full_answer)
            safe_update_placeholder(full_answer, show_cursor=False)
        except Exception as stream_error:
            # å¦‚æœæµå¼è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨éæµå¼æ¨¡å¼
            st.warning(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨éæµå¼æ¨¡å¼: {stream_error}")
            response = ollama.chat(
                model=MODEL,
                messages=messages
            )
            full_answer = response["message"]["content"]
            full_answer = filter_promotional_content(full_answer)
            safe_update_placeholder(full_answer, show_cursor=False)
        
    except Exception as e:
        error_msg = str(e)
        st.error(
            f"âŒ Ollama æ¨ç†å¤±è´¥: {error_msg}\n\n"
            "è¯·æ£€æŸ¥ï¼š\n"
            f"1. Ollama æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ: ollama serve\n"
            f"2. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½: ollama pull {MODEL}\n"
            "3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
        )
        
        try:
            if placeholder is not None:
                placeholder.error(f"âŒ ç”Ÿæˆå¤±è´¥: {error_msg}")
            else:
                st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {error_msg}")
        except Exception:
            st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {error_msg}")
    
    return full_answer

from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from core import (
    PathConstants,
    RAGConstants,
    default_logger,
)


class VideoRAGSystem:
    """åŸºäºLangChainçš„è§†é¢‘RAGç³»ç»Ÿ"""
    
    def __init__(self, embedding_model: Embeddings):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        :param embedding_model: ç”¨äºå‘é‡åŒ–çš„æ¨¡å‹å®ä¾‹
        """
        self.embedding_model = embedding_model
        self.vectorstore: Optional[FAISS] = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=RAGConstants.DEFAULT_CHUNK_SIZE,
            chunk_overlap=RAGConstants.DEFAULT_CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        self.index_path = PathConstants.FAISS_INDEX_DIR
        self.logger = default_logger
    
    def create_documents_from_segments(self, segments: List[Dict]) -> List[Document]:
        """
        å°†Whisperè½¬å½•ç‰‡æ®µè½¬æ¢ä¸ºLangChain Documentå¯¹è±¡
        ä¿ç•™æ—¶é—´æˆ³å…ƒæ•°æ®
        """
        documents = []
        for seg in segments:
            # åˆ›å»ºåŒ…å«æ—¶é—´æˆ³ä¿¡æ¯çš„æ–‡æ¡£
            doc = Document(
                page_content=seg['text'],
                metadata={
                    'start_time': seg['start'],
                    'end_time': seg['end'],
                    'timestamp': f"{int(seg['start']//60)}:{int(seg['start']%60):02d}"
                }
            )
            documents.append(doc)
        return documents
    
    def build_vector_store(
        self,
        segments: List[Dict],
        video_id: Optional[str] = None
    ) -> None:
        """
        æ„å»ºå‘é‡å­˜å‚¨
        
        Args:
            segments: Whisperè½¬å½•ç‰‡æ®µ
            video_id: è§†é¢‘IDï¼Œç”¨äºå¤šè§†é¢‘åœºæ™¯
        
        Raises:
            ValueError: å¦‚æœsegmentsä¸ºç©º
        """
        if not segments:
            raise ValueError("æ— æ³•æ„å»ºå‘é‡ç´¢å¼•ï¼šè½¬å½•ç‰‡æ®µä¸ºç©º")
        
        # è½¬æ¢ä¸ºDocumentå¯¹è±¡
        documents = self.create_documents_from_segments(segments)
        
        # æŒ‰æ—¶é—´çª—å£åˆå¹¶ç›¸å…³ç‰‡æ®µï¼ˆå¯é€‰ï¼‰
        # å¦‚æœç‰‡æ®µå¤ªçŸ­ï¼Œå¯ä»¥åˆå¹¶ç›¸é‚»ç‰‡æ®µ
        merged_docs = self._merge_short_segments(documents)
        
        # åˆ†å‰²é•¿æ–‡æ¡£
        split_docs = self.text_splitter.split_documents(merged_docs)
        
        if not split_docs:
            raise ValueError("æ— æ³•æ„å»ºå‘é‡ç´¢å¼•ï¼šæ–‡æ¡£åˆ†å‰²åä¸ºç©º")
        
        # æ„å»ºFAISSå‘é‡å­˜å‚¨
        self.vectorstore = FAISS.from_documents(
            documents=split_docs,
            embedding=self.embedding_model
        )
        
        # ä¿å­˜ç´¢å¼•
        index_dir = self._get_index_path(video_id)
        index_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ç´¢å¼•
        self.vectorstore.save_local(str(index_dir))
        self.logger.info("å‘é‡ç´¢å¼•å·²æ„å»º: %s (segments=%s)", index_dir, len(split_docs))
        
        # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
        faiss_file = index_dir / "index.faiss"
        pkl_file = index_dir / "index.pkl"
        if faiss_file.exists() and pkl_file.exists():
            st.success(f"âœ… å‘é‡ç´¢å¼•å·²æ„å»ºå¹¶ä¿å­˜åˆ° {index_dir}")
            self.logger.info("å‘é‡ç´¢å¼•å·²éªŒè¯: %s", index_dir)
        else:
            st.warning(f"âš ï¸ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œä½†ä¿å­˜éªŒè¯å¤±è´¥: {index_dir}")
            self.logger.warning("å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œä½†ä¿å­˜éªŒè¯å¤±è´¥: %s", index_dir)
    
    def _merge_short_segments(
        self,
        documents: List[Document],
        min_length: Optional[int] = None
    ) -> List[Document]:
        """
        åˆå¹¶è¿‡çŸ­çš„ç‰‡æ®µ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            min_length: æœ€å°é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä½¿ç”¨ RAGConstants.MIN_SEGMENT_LENGTH
        """
        if min_length is None:
            min_length = RAGConstants.MIN_SEGMENT_LENGTH
        
        merged = []
        current_doc = None
        
        for doc in documents:
            if len(doc.page_content) < min_length and current_doc:
                # åˆå¹¶åˆ°å½“å‰æ–‡æ¡£
                current_doc.page_content += " " + doc.page_content
                current_doc.metadata['end_time'] = doc.metadata['end_time']
            else:
                if current_doc:
                    merged.append(current_doc)
                current_doc = doc
        
        if current_doc:
            merged.append(current_doc)
        return merged
    
    def _get_index_path(self, video_id: Optional[str] = None) -> Path:
        """è·å–ç´¢å¼•ç›®å½•è·¯å¾„"""
        if video_id:
            return Path(f"{self.index_path}_{video_id}")
        return Path(self.index_path)

    def _is_index_valid(self, index_dir: Path) -> bool:
        """æ£€æŸ¥ç´¢å¼•ç›®å½•æ˜¯å¦æœ‰æ•ˆ"""
        if not index_dir.exists() or not index_dir.is_dir():
            return False
        faiss_file = index_dir / "index.faiss"
        pkl_file = index_dir / "index.pkl"
        return faiss_file.exists() and pkl_file.exists()

    def load_vector_store(self, video_id: Optional[str] = None) -> bool:
        """
        åŠ è½½å·²ä¿å­˜çš„å‘é‡å­˜å‚¨
        
        Args:
            video_id: è§†é¢‘IDï¼Œç”¨äºå¤šè§†é¢‘åœºæ™¯
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        index_dir = self._get_index_path(video_id)
        index_path = str(index_dir)
        
        if not self._is_index_valid(index_dir):
            return False
        
        try:
            self.vectorstore = FAISS.load_local(
                str(index_path),
                self.embedding_model,
                allow_dangerous_deserialization=True
            )
            self.logger.info("æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•: %s", index_path)
            return True
        except FileNotFoundError:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸éœ€è¦è­¦å‘Š
            return False
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶æŸåï¼‰ï¼Œè®°å½•è­¦å‘Š
            st.warning(f"âš ï¸ æ— æ³•åŠ è½½å‘é‡ç´¢å¼•: {e}")
            self.logger.warning("æ— æ³•åŠ è½½å‘é‡ç´¢å¼•: %s", e, exc_info=True)
            return False
    
    def retrieve_relevant_context(
        self,
        query: str,
        top_k: int = RAGConstants.DEFAULT_TOP_K,
        score_threshold: float = RAGConstants.DEFAULT_SCORE_THRESHOLD,
        use_mmr: bool = False,
        fetch_k: int = 15,
        lambda_mult: float = 0.5,
        use_hybrid: bool = False
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        :param query: æŸ¥è¯¢é—®é¢˜
        :param top_k: è¿”å›top-kä¸ªç›¸å…³ç‰‡æ®µ
        :param score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆL2è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰
        :param use_mmr: æ˜¯å¦å¯ç”¨ MMR æ£€ç´¢ä»¥æå‡å¤šæ ·æ€§
        :param fetch_k: MMR é¢„æ£€ç´¢æ•°é‡
        :param lambda_mult: MMR å¤šæ ·æ€§å‚æ•°
        :param use_hybrid: æ˜¯å¦æ··åˆè¯­ä¹‰+å…³é”®è¯æ£€ç´¢
        :return: ç›¸å…³ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬ã€æ—¶é—´æˆ³ç­‰ä¿¡æ¯
        """
        if self.vectorstore is None:
            return []
        
        docs_with_scores: Sequence[Any] = []

        if use_mmr:
            try:
                mmr_docs = self.vectorstore.max_marginal_relevance_search(
                    query,
                    k=top_k,
                    fetch_k=fetch_k,
                    lambda_mult=lambda_mult,
                )
                docs_with_scores = [(doc, 0.0) for doc in mmr_docs]
            except Exception as e:
                self.logger.warning("MMR æ£€ç´¢å¤±è´¥ï¼Œå›é€€æ™®é€šæ£€ç´¢: %s", e)

        if not docs_with_scores:
            docs_with_scores = self.vectorstore.similarity_search_with_score(
                query,
                k=top_k
            )

        keyword_docs: List[Document] = []
        if use_hybrid:
            keyword_docs = self._keyword_search(query, top_k=top_k)
        
        # è¿‡æ»¤é«˜åˆ†ç»“æœï¼ˆL2è·ç¦»ï¼Œè¶Šå°è¶Šå¥½ï¼‰å¹¶æ ¼å¼åŒ–
        contexts = []
        for doc, score in docs_with_scores:
            similarity_score = 1 / (1 + score) if score > 0 else 1.0
            if score <= score_threshold or use_mmr:
                contexts.append(self._doc_to_context(doc, similarity_score))

        if keyword_docs:
            keyword_contexts = [
                self._doc_to_context(doc, score=0.0) for doc in keyword_docs
            ]
            contexts.extend(keyword_contexts)

        # å»é‡å¹¶æŒ‰åˆ†æ•°æ’åº
        return self._dedup_contexts(contexts)[:top_k]
    
    def retrieve_around_timestamp(
        self,
        timestamp: float,
        window: int = 5,
        query: Optional[str] = None,
        top_k: int = RAGConstants.DEFAULT_TOP_K
    ) -> Dict[str, Any]:
        """
        ç»“åˆæ—¶é—´æˆ³å’Œè¯­ä¹‰æ£€ç´¢
        :param timestamp: ç›®æ ‡æ—¶é—´æˆ³
        :param window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        :param query: å¯é€‰çš„é—®é¢˜ï¼Œç”¨äºè¯­ä¹‰è¿‡æ»¤
        :return: ç›¸å…³ä¸Šä¸‹æ–‡
        """
        if self.vectorstore is None:
            return {
                'text': '',
                'contexts': [],
                'retrieval_type': 'no_index'
            }
        
        # å¦‚æœæä¾›äº†æŸ¥è¯¢ï¼Œå…ˆè¿›è¡Œè¯­ä¹‰æ£€ç´¢
        if query:
            semantic_results = self.retrieve_relevant_context(
                query,
                top_k=max(top_k, 5),
                use_mmr=True,
                use_hybrid=True
            )
            # è¿‡æ»¤å‡ºæ—¶é—´æˆ³é™„è¿‘çš„ç‰‡æ®µ
            time_filtered = [
                ctx for ctx in semantic_results
                if abs(ctx['start_time'] - timestamp) <= window * 2
            ]
            if time_filtered:
                return {
                    'text': ' '.join([ctx['text'] for ctx in time_filtered]),
                    'contexts': time_filtered,
                    'retrieval_type': 'semantic_temporal',
                    'timestamp': timestamp,
                    'window': window
                }

        # æ—¶é—´çª—å£æ£€ç´¢ï¼šä» docstore è¿‡æ»¤é è¿‘æ—¶é—´æˆ³çš„æ–‡æ¡£
        nearby_contexts = self._retrieve_by_time_window(timestamp, window, limit=top_k)
        if nearby_contexts:
            return {
                'text': ' '.join([ctx['text'] for ctx in nearby_contexts]),
                'contexts': nearby_contexts,
                'retrieval_type': 'temporal_only',
                'timestamp': timestamp,
                'window': window
            }

        # å›é€€ï¼šè¿”å›ç©ºç»“æœï¼Œä½†æ ‡è®°åŸå› 
        return {
            'text': '',
            'contexts': [],
            'retrieval_type': 'temporal_only',
            'note': 'æœªæ‰¾åˆ°æ—¶é—´çª—å£å†…çš„ç‰‡æ®µ'
        }
    
    def cleanup_invalid_indices(self, keep_signatures: Optional[List[str]] = None):
        """
        æ¸…ç†æ— æ•ˆæˆ–è¿‡æœŸçš„ç´¢å¼•æ–‡ä»¶
        
        Args:
            keep_signatures: è¦ä¿ç•™çš„è§†é¢‘ç­¾ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        base_path = Path(self.index_path).parent
        pattern = "faiss_index*"
        
        cleaned_count = 0
        for index_dir in base_path.glob(pattern):
            if not index_dir.is_dir():
                continue
            
            # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å®Œæ•´
            if not self._is_index_valid(index_dir):
                try:
                    import shutil
                    shutil.rmtree(index_dir)
                    cleaned_count += 1
                    self.logger.info("å·²åˆ é™¤æ— æ•ˆç´¢å¼•: %s", index_dir)
                except Exception as e:
                    st.warning(f"âš ï¸ æ— æ³•åˆ é™¤æ— æ•ˆç´¢å¼• {index_dir}: {e}")
                    self.logger.warning("æ— æ³•åˆ é™¤æ— æ•ˆç´¢å¼• %s: %s", index_dir, e, exc_info=True)
            
            # å¦‚æœæŒ‡å®šäº†è¦ä¿ç•™çš„ç­¾åï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ é™¤
            elif keep_signatures:
                dir_name = index_dir.name
                if dir_name.startswith("faiss_index_"):
                    signature = dir_name.replace("faiss_index_", "")
                    if signature not in keep_signatures:
                        try:
                            import shutil
                            shutil.rmtree(index_dir)
                            cleaned_count += 1
                            self.logger.info("å·²åˆ é™¤è¿‡æœŸç´¢å¼•: %s", index_dir)
                        except Exception as e:
                            st.warning(f"âš ï¸ æ— æ³•åˆ é™¤è¿‡æœŸç´¢å¼• {index_dir}: {e}")
                            self.logger.warning("æ— æ³•åˆ é™¤è¿‡æœŸç´¢å¼• %s: %s", index_dir, e, exc_info=True)
        
        if cleaned_count > 0:
            st.info(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} ä¸ªæ— æ•ˆç´¢å¼•")
    
    def _doc_to_context(self, doc: Document, score: float) -> Dict[str, Any]:
        """å°† Document è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å­—å…¸"""
        return {
            'text': doc.page_content,
            'start_time': doc.metadata.get('start_time', 0),
            'end_time': doc.metadata.get('end_time', 0),
            'timestamp': doc.metadata.get('timestamp', ''),
            'score': float(score)
        }

    def _dedup_contexts(self, contexts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰æ—¶é—´æˆ³å’Œæ–‡æœ¬å»é‡"""
        seen = set()
        unique_contexts = []
        for ctx in contexts:
            key = (ctx.get('start_time'), ctx.get('timestamp'), ctx.get('text'))
            if key in seen:
                continue
            seen.add(key)
            unique_contexts.append(ctx)
        return sorted(unique_contexts, key=lambda c: c.get('score', 0), reverse=True)

    def _keyword_search(self, query: str, top_k: int) -> List[Document]:
        """ç®€å•å…³é”®è¯æ£€ç´¢ï¼Œä½œä¸ºè¯­ä¹‰æ£€ç´¢è¡¥å……"""
        if self.vectorstore is None or not hasattr(self.vectorstore, "docstore"):
            return []
        tokens = {tok.lower() for tok in query.split() if tok}
        scored_docs = []
        try:
            for doc in self.vectorstore.docstore._dict.values():
                content = doc.page_content.lower()
                overlap = sum(1 for tok in tokens if tok in content)
                if overlap:
                    scored_docs.append((doc, overlap))
        except Exception as e:
            self.logger.debug("å…³é”®è¯æ£€ç´¢å¤±è´¥: %s", e, exc_info=True)
            return []
        scored_docs.sort(key=lambda item: item[1], reverse=True)
        return [doc for doc, _ in scored_docs[:top_k]]

    def _retrieve_by_time_window(
        self,
        timestamp: float,
        window: int,
        limit: int
    ) -> List[Dict[str, Any]]:
        """åŸºäºæ—¶é—´çª—å£çš„ç®€å•æ£€ç´¢"""
        if self.vectorstore is None or not hasattr(self.vectorstore, "docstore"):
            return []
        candidates: List[Document] = []
        try:
            for doc in self.vectorstore.docstore._dict.values():
                start_time = doc.metadata.get('start_time', 0)
                if abs(start_time - timestamp) <= window:
                    candidates.append(doc)
        except Exception as e:
            self.logger.debug("æ—¶é—´çª—å£æ£€ç´¢å¤±è´¥: %s", e, exc_info=True)
            return []
        candidates.sort(key=lambda d: d.metadata.get('start_time', 0))
        contexts = [self._doc_to_context(doc, score=0.0) for doc in candidates]
        return contexts[:limit]
